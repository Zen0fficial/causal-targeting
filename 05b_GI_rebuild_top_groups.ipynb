{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d2ace6",
   "metadata": {},
   "source": [
    "# 05b Rebuild Top Groups and Refit Models\n",
    "\n",
    "This notebook rebuilds a model for the top groups selected in `03a_GI_validate_CATE_estimators.ipynb` / `04a_GI_rank_CATE_estimators.ipynb`.\n",
    "\n",
    "- Identify units that fall in the top quantile [0.8, 1] across 12 CV folds with frequency >= 0.33.\n",
    "- Compute Neyman t-statistic and p-value for that subgroup.\n",
    "- Retune estimators with 4-fold CV and refit on all units.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d431f28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to import duecredit due to No module named 'duecredit'\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [10:06:58] WARNING: /Users/runner/work/xgboost/xgboost/src/gbm/../common/error_msg.h:80: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.7.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.7.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator Lasso from version 1.7.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.7.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegressionCV from version 1.7.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded optimal ensemble from 04b: ['x_xgb', 'causal_tree_1', 'x_rf', 't_rf', 'x_logistic']\n",
      "  Configuration: Q*=Q3, k*=10\n",
      "Loaded 5 top estimator results from output/analysis/fausebal/fausebal_05b_top_results.pkl\n",
      "Fitted 5 top estimators on all trainval units.\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from methods.causal_functions import get_subgroup_t_statistic, get_Neyman_ATE, get_subgroup_CATE_std\n",
    "from methods.cate_estimator_wrappers import CATEEstimatorResults, XLearnerWrapper\n",
    "\n",
    "\n",
    "# Config\n",
    "outcome = \"fausebal\"  # adjust as needed\n",
    "DATA_DIR = Path(\"output/analysis\")\n",
    "PARAMS_DIR = Path(\"output/params\")\n",
    "ANALYSIS_DIR = DATA_DIR / outcome\n",
    "PARAMS_PATH = PARAMS_DIR / outcome / f\"{outcome}_tuned_params.pkl\"\n",
    "IMPUTATION_META = PARAMS_DIR / outcome / \"analysis_imputation_meta.pkl\"\n",
    "with open(IMPUTATION_META, 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "\n",
    "features = meta[\"features\"]\n",
    "treatment_var = meta.get(\"treatment_var\", \"TREATED\")\n",
    "all_outcomes = meta.get(\"outcomes\", [outcome])\n",
    "\n",
    "# Load fitted libraries and top estimator names from 03a/04a cache\n",
    "FITTED_LIBS_PATH = ANALYSIS_DIR / f\"{outcome}_fitted_libraries.pkl\"\n",
    "\n",
    "fitted_libraries = joblib.load(FITTED_LIBS_PATH)\n",
    "# Load optimal ensemble from 04b\n",
    "INTERMEDIATE_PATH = Path(\"output/intermediate/grid_search\")\n",
    "optimal_config_path = INTERMEDIATE_PATH / f\"{outcome}_optimal_config.pkl\"\n",
    "\n",
    "optimal_config = joblib.load(optimal_config_path)\n",
    "top_estimator_names = optimal_config[\"estimators\"]\n",
    "print(f\"Loaded optimal ensemble from 04b: {top_estimator_names}\")\n",
    "print(f\"  Configuration: Q*={optimal_config['Q_star']}, k*={optimal_config['k_star']}\")\n",
    "lib_pert_none = fitted_libraries[\"pert_none\"]\n",
    "n_samples_tv = len(next(iter(lib_pert_none.values())).y)\n",
    "train_indices = np.arange(n_samples_tv)\n",
    "val_indices = np.arange(n_samples_tv)\n",
    "\n",
    "# Train fold-free results and compute average ITEs on trainval\n",
    "TOP_RESULTS_PKL = ANALYSIS_DIR / f\"{outcome}_05b_top_results.pkl\"\n",
    "if TOP_RESULTS_PKL.exists():\n",
    "    top_results = joblib.load(TOP_RESULTS_PKL)\n",
    "    print(f\"Loaded {len(top_results)} top estimator results from {TOP_RESULTS_PKL}\")\n",
    "else:\n",
    "    top_results = {}\n",
    "    for est_name in top_estimator_names:\n",
    "        if est_name not in lib_pert_none:\n",
    "            continue\n",
    "        est = lib_pert_none[est_name]\n",
    "        res = CATEEstimatorResults(train_indices, val_indices, est, save_metalearner=True)\n",
    "        top_results[est_name] = res\n",
    "    joblib.dump(top_results, TOP_RESULTS_PKL)\n",
    "    print(f\"Saved {len(top_results)} top estimator results to {TOP_RESULTS_PKL}\")\n",
    "\n",
    "tau_stack_tv = np.vstack([res.tau for res in top_results.values()]) if len(top_results) > 0 else np.empty((0, n_samples_tv))\n",
    "tau_avg_tv = tau_stack_tv.mean(axis=0) if tau_stack_tv.size > 0 else np.zeros(n_samples_tv)\n",
    "print(f\"Fitted {len(top_results)} top estimators on all trainval units.\")\n",
    "\n",
    "# Helper to predict on new data with fold-free fitted result\n",
    "def _predict_on_new(result, estimator_wrapper, X_new):\n",
    "    Xn = np.asarray(X_new, dtype=float)\n",
    "    if getattr(result, \"_selector\", None) is not None:\n",
    "        n_features = estimator_wrapper.X.shape[1]\n",
    "        col_names = [f\"x_{i}\" for i in range(n_features)]\n",
    "        try:\n",
    "            X_df = pd.DataFrame(Xn, columns=pd.Index(col_names))\n",
    "            Xn = result._selector.transform(X_df).values\n",
    "        except Exception:\n",
    "            pass\n",
    "    if isinstance(estimator_wrapper, XLearnerWrapper):\n",
    "        p = np.mean(estimator_wrapper.t) * np.ones(Xn.shape[0])\n",
    "        pred = result.meta_learner.predict(Xn, p=p)\n",
    "    else:\n",
    "        pred = result.meta_learner.predict(Xn)\n",
    "    return np.asarray(pred).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "432d75f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_samples': 28830, 'subgroup_size': 2883, 'ATE': 0.13813965710413545, 't_stat': 7.798796551007339}\n"
     ]
    }
   ],
   "source": [
    "# Build [0.9, 1] subgroup by ranking averaged ITEs on trainval\n",
    "# dir_neg=False selects largest effects; True selects smallest effects\n",
    "dir_neg = False\n",
    "q_bot, q_top = (0.9, 1.0)\n",
    "\n",
    "n_samples = len(tau_avg_tv)\n",
    "\n",
    "# Load aligned trainval data to get y and t\n",
    "trainval_df = pd.read_csv(ANALYSIS_DIR / \"trainval_data.csv\")\n",
    "y = trainval_df[outcome].values\n",
    "if 'TREATED' in trainval_df.columns:\n",
    "    t = trainval_df['TREATED'].values.astype(int)\n",
    "elif treatment_var in trainval_df.columns:\n",
    "    t = trainval_df[treatment_var].values.astype(int)\n",
    "else:\n",
    "    raise KeyError(\"trainval data must include 'TREATED' or treatment_var column\")\n",
    "\n",
    "# Threshold by quantiles of tau_avg_tv\n",
    "from scipy.stats import norm\n",
    "if not dir_neg:\n",
    "    thr_tv = np.quantile(tau_avg_tv, q_bot)\n",
    "    subgroup_indicator = (tau_avg_tv >= thr_tv)\n",
    "else:\n",
    "    thr_tv = np.quantile(tau_avg_tv, q_top)\n",
    "    subgroup_indicator = (tau_avg_tv <= thr_tv)\n",
    "\n",
    "ATE = get_Neyman_ATE(y[subgroup_indicator], t[subgroup_indicator]) if subgroup_indicator.any() else np.nan\n",
    "# t-stat vs zero to align sign with ATE\n",
    "if subgroup_indicator.any() and (t[subgroup_indicator].sum() > 0) and ((1 - t[subgroup_indicator]).sum() > 0):\n",
    "    CATE_std = get_subgroup_CATE_std(y, t, subgroup_indicator)\n",
    "    t_stat = ATE / CATE_std if (np.isfinite(CATE_std) and CATE_std > 0) else np.nan\n",
    "    p_value = 2 * (1 - norm.cdf(abs(t_stat))) if np.isfinite(t_stat) else np.nan\n",
    "else:\n",
    "    t_stat, p_value = np.nan, np.nan\n",
    "\n",
    "print({\n",
    "    \"n_samples\": int(n_samples),\n",
    "    \"subgroup_size\": int(np.asarray(subgroup_indicator, dtype=bool).sum()),\n",
    "    \"ATE\": None if (ATE is np.nan or not np.isfinite(ATE)) else float(ATE),\n",
    "    \"t_stat\": None if (t_stat is np.nan or not np.isfinite(t_stat)) else float(t_stat),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d214cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_holdout': 7208, 'subgroup_size_holdout': 721, 'ATE_holdout': -0.017937944408532636, 't_stat_holdout': -0.503470527552398}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: overflow encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: overflow encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: overflow encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: overflow encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: overflow encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: overflow encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/zenofficial/Documents/statistics/pcs/document/projects/causal-targeting-main/venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n"
     ]
    }
   ],
   "source": [
    "# Holdout subgroup via averaged predictions from fold-free top estimators\n",
    "holdout_df = pd.read_csv(ANALYSIS_DIR / \"holdout_data.csv\")\n",
    "X_hold = holdout_df[features].copy().apply(pd.to_numeric, errors='coerce').fillna(0.0).values\n",
    "y_hold = holdout_df[outcome].values\n",
    "if 'TREATED' in holdout_df.columns:\n",
    "    t_hold = holdout_df['TREATED'].values.astype(int)\n",
    "elif treatment_var in holdout_df.columns:\n",
    "    t_hold = holdout_df[treatment_var].values.astype(int)\n",
    "else:\n",
    "    raise KeyError(\"Holdout data must include 'TREATED' or treatment_var column\")\n",
    "\n",
    "# Predict ITEs on holdout for each fitted top estimator and average\n",
    "if len(top_results) > 0:\n",
    "    tau_hold_stack = []\n",
    "    for est_name, res in top_results.items():\n",
    "        est_wrapper = lib_pert_none[est_name]\n",
    "        tau_hold_stack.append(_predict_on_new(res, est_wrapper, X_hold))\n",
    "    tau_avg_hold = np.mean(np.vstack(tau_hold_stack), axis=0)\n",
    "else:\n",
    "    tau_avg_hold = np.zeros(len(holdout_df))\n",
    "\n",
    "# Threshold by quantiles of tau_avg_hold (separate holdout ranking)\n",
    "from scipy.stats import norm\n",
    "if not dir_neg:\n",
    "    thr_hold = np.quantile(tau_avg_hold, q_bot)\n",
    "    subgroup_hold = (tau_avg_hold >= thr_hold)\n",
    "else:\n",
    "    thr_hold = np.quantile(tau_avg_hold, q_top)\n",
    "    subgroup_hold = (tau_avg_hold <= thr_hold)\n",
    "\n",
    "ATE_hold = get_Neyman_ATE(y_hold[subgroup_hold], t_hold[subgroup_hold]) if subgroup_hold.any() else np.nan\n",
    "# t-stat vs zero to align sign with ATE\n",
    "if subgroup_hold.any() and (t_hold[subgroup_hold].sum() > 0) and ((1 - t_hold[subgroup_hold]).sum() > 0):\n",
    "    CATE_std_hold = get_subgroup_CATE_std(y_hold, t_hold, subgroup_hold)\n",
    "    t_stat_hold = ATE_hold / CATE_std_hold if (np.isfinite(CATE_std_hold) and CATE_std_hold > 0) else np.nan\n",
    "    p_value_hold = 2 * (1 - norm.cdf(abs(t_stat_hold))) if np.isfinite(t_stat_hold) else np.nan\n",
    "else:\n",
    "    t_stat_hold, p_value_hold = np.nan, np.nan\n",
    "\n",
    "print({\n",
    "    \"n_holdout\": int(len(holdout_df)),\n",
    "    \"subgroup_size_holdout\": int(np.asarray(subgroup_hold, dtype=bool).sum()),\n",
    "    \"ATE_holdout\": None if (ATE_hold is np.nan or not np.isfinite(ATE_hold)) else float(ATE_hold),\n",
    "    \"t_stat_holdout\": None if (t_stat_hold is np.nan or not np.isfinite(t_stat_hold)) else float(t_stat_hold),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8779e65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_all': 36038, 'subgroup_size_all': 3604, 'ATE_all': 0.10690676263113891, 't_stat_all': 6.7277390592105935}\n"
     ]
    }
   ],
   "source": [
    "# Pooled stats (trainval + holdout)\n",
    "y_all = np.asarray(trainval_df[outcome].values, dtype=float)\n",
    "t_all = np.asarray(\n",
    "    trainval_df['TREATED'].values.astype(int) if 'TREATED' in trainval_df.columns\n",
    "    else trainval_df[treatment_var].values.astype(int),\n",
    "    dtype=int,\n",
    ")\n",
    "\n",
    "subgroup_all = np.concatenate([\n",
    "    np.asarray(subgroup_indicator, dtype=bool),\n",
    "    np.asarray(subgroup_hold, dtype=bool),\n",
    "])\n",
    "y_all_pool = np.concatenate([y_all, np.asarray(y_hold, dtype=float)])\n",
    "t_all_pool = np.concatenate([t_all, np.asarray(t_hold, dtype=int)])\n",
    "\n",
    "from scipy.stats import norm\n",
    "ATE_all = get_Neyman_ATE(y_all_pool[subgroup_all], t_all_pool[subgroup_all]) if subgroup_all.any() else np.nan\n",
    "# t-stat vs zero to align sign with ATE\n",
    "if subgroup_all.any() and (t_all_pool[subgroup_all].sum() > 0) and ((1 - t_all_pool[subgroup_all]).sum() > 0):\n",
    "    CATE_std_all = get_subgroup_CATE_std(y_all_pool, t_all_pool, subgroup_all)\n",
    "    t_stat_all = ATE_all / CATE_std_all if (np.isfinite(CATE_std_all) and CATE_std_all > 0) else np.nan\n",
    "    p_value_all = 2 * (1 - norm.cdf(abs(t_stat_all))) if np.isfinite(t_stat_all) else np.nan\n",
    "else:\n",
    "    t_stat_all, p_value_all = np.nan, np.nan\n",
    "\n",
    "print({\n",
    "    \"n_all\": int(len(y_all_pool)),\n",
    "    \"subgroup_size_all\": int(np.sum(subgroup_all)),\n",
    "    \"ATE_all\": None if (ATE_all is np.nan or np.isnan(ATE_all)) else float(ATE_all),\n",
    "    \"t_stat_all\": None if (t_stat_all is np.nan or not np.isfinite(t_stat_all)) else float(t_stat_all),\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
