{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Turkey Bank Overdraft Analysis - Master Notebook\n",
        "\n",
        "**Unshrouding: Evidence from Bank Overdrafts in Turkey**  \n",
        "Python Replication - June 2017  \n",
        "\n",
        "This master notebook orchestrates the entire analysis pipeline, equivalent to the original `0_master_all.do` Stata script.\n",
        "\n",
        "## Analysis Pipeline\n",
        "\n",
        "1. **Setup**\n",
        "2. **Data Cleaning**  \n",
        "3. **Main Analysis Tables**\n",
        "4. **Appendix Tables**\n",
        "5. **Figures**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Master Analysis Started: 2025-08-28 00:39:43.425192\n",
            "Project Root: /Users/zenofficial/Documents/statistics/pcs/turkey_python_analysis\n",
            "Study: Unshrouding: Evidence from Bank Overdrafts in Turkey\n",
            "\n",
            "Analysis Configuration:\n",
            "  Data Cleaning: True\n",
            "  Main Analysis: True\n",
            "  Excel Output: True\n",
            "  Figures: True\n",
            "\n",
            "Loading data from: /Users/zenofficial/Documents/statistics/pcs/Turkey Dataverse Files/Turkey Dataverse Files/Data\n",
            "Loading datasets directly with pyreadstat...\n",
            "==================================================\n",
            "Loading all...\n",
            "✓ all: 1,836,354 rows × 42 columns\n",
            "Loading dailydata...\n",
            "✓ dailydata: 1,195,591 rows × 3 columns\n",
            "Loading campaign_and_postcampaign...\n",
            "✓ campaign_and_postcampaign: 2,484,000 rows × 49 columns\n",
            "Loading campaign_and_postcampaign_controlmerged...\n",
            "✓ campaign_and_postcampaign_controlmerged: 3,389,441 rows × 54 columns\n",
            "Loading orthogonality...\n",
            "✓ orthogonality: 108,000 rows × 77 columns\n",
            "Loading data_group...\n",
            "✓ data_group: 2,808,000 rows × 35 columns\n",
            "Loading data_group_interactions...\n",
            "✓ data_group_interactions: 2,808,000 rows × 519 columns\n",
            "\n",
            "Successfully loaded 7 datasets: ['all', 'dailydata', 'campaign_and_postcampaign', 'campaign_and_postcampaign_controlmerged', 'orthogonality', 'data_group', 'data_group_interactions']\n"
          ]
        }
      ],
      "source": [
        "# Setup and imports\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.append(str(project_root / 'config'))\n",
        "\n",
        "# Import configuration\n",
        "import config\n",
        "import pyreadstat\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "print(f\"Master Analysis Started: {datetime.now()}\")\n",
        "print(f\"Project Root: {project_root}\")\n",
        "print(f\"Study: {config.STUDY_TITLE}\")\n",
        "\n",
        "# Configuration control flags (equivalent to Stata locals)\n",
        "run_data_cleaning = True  \n",
        "run_analysis = True\n",
        "\n",
        "# Output options\n",
        "create_excel_output = True\n",
        "create_figures = True\n",
        "verbose = True\n",
        "\n",
        "print(\"\\nAnalysis Configuration:\")\n",
        "print(f\"  Data Cleaning: {run_data_cleaning}\")\n",
        "print(f\"  Main Analysis: {run_analysis}\")\n",
        "print(f\"  Excel Output: {create_excel_output}\")\n",
        "print(f\"  Figures: {create_figures}\")\n",
        "\n",
        "# Load all datasets directly from original files using pyreadstat\n",
        "print(f\"\\nLoading data from: {config.ORIGINAL_DATA_DIR}\")\n",
        "print(\"Loading datasets directly with pyreadstat...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "datasets = {}\n",
        "for name, file_path in config.DATASETS.items():\n",
        "    if file_path.exists():\n",
        "        print(f\"Loading {name}...\")\n",
        "        try:\n",
        "            df, meta = pyreadstat.read_dta(str(file_path))\n",
        "            datasets[name] = df\n",
        "            print(f\"✓ {name}: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Failed to load {name}: {e}\")\n",
        "    else:\n",
        "        print(f\"✗ File not found: {file_path}\")\n",
        "\n",
        "print(f\"\\nSuccessfully loaded {len(datasets)} datasets: {list(datasets.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "COMPLETE ORTHOGONALITY DATA PREPARATION\n",
            "Replicating: data7_orthogonality.do\n",
            "============================================================\n",
            "Starting with 'all' dataset: 1,836,354 rows\n",
            "\n",
            "Step 1: Filtering to pre-campaign period...\n",
            "✓ Filtered: 1,836,354 → 1,296,000 rows (540,354 removed)\n",
            "✓ Sorted by id and date\n"
          ]
        }
      ],
      "source": [
        "# Complete Orthogonality Data Preparation - Exact Stata Replication\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPLETE ORTHOGONALITY DATA PREPARATION\")\n",
        "print(\"Replicating: data7_orthogonality.do\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load 'all' dataset\n",
        "if 'all' not in datasets:\n",
        "    raise ValueError(\"'all' dataset not found in loaded datasets\")\n",
        "\n",
        "df = datasets['all'].copy()\n",
        "print(f\"Starting with 'all' dataset: {df.shape[0]:,} rows\")\n",
        "\n",
        "# Step 1: Filter to pre-campaign period\n",
        "# Stata: keep if (date <= ym(2012,8) & date >= ym(2011,9))\n",
        "print(\"\\nStep 1: Filtering to pre-campaign period...\")\n",
        "initial_rows = len(df)\n",
        "df = df[(df['date'] >= 621) & (df['date'] <= 632)].copy()  # ym(2011,9) to ym(2012,8)\n",
        "print(f\"✓ Filtered: {initial_rows:,} → {len(df):,} rows ({initial_rows - len(df):,} removed)\")\n",
        "\n",
        "# Sort by id and date\n",
        "df = df.sort_values(['id', 'date'])\n",
        "print(\"✓ Sorted by id and date\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 3: Creating aggregated variables...\n",
            "Checking column data types:\n",
            "  a_total: float64\n",
            "  a_deposit: float64\n",
            "  payment: float64\n",
            "  debt_tot: object\n",
            "  transnum: object\n",
            "  var3: int64\n",
            "  faamount: float64\n",
            "\n",
            "Converting columns to numeric...\n",
            "  Converted a_total to numeric\n",
            "  Converted a_deposit to numeric\n",
            "  Converted payment to numeric\n",
            "  Converted debt_tot to numeric\n",
            "  Converted transnum to numeric\n",
            "✓ Created transactions from transnum (mean: 1.64)\n",
            "✓ Created assets from a_total (mean: 584.12)\n",
            "✓ Created deposits from a_deposit (mean: 342.38)\n",
            "✓ Created paymentmean from payment (mean: 6.96)\n",
            "✓ Created debt from debt_tot (mean: 283.39)\n",
            "✓ Created pastuse: 32,865 customers with past usage\n",
            "  Distribution: {0: 75135, 1: 32865}\n",
            "✓ Created autobillpay: 3,715 customers with autobillpay\n",
            "✓ All aggregated variables created\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Create aggregated variables (Stata egen commands)\n",
        "print(\"\\nStep 3: Creating aggregated variables...\")\n",
        "\n",
        "# Check data types first\n",
        "print(\"Checking column data types:\")\n",
        "cols_to_check = ['a_total', 'a_deposit', 'payment', 'debt_tot', 'transnum', 'var3', 'faamount']\n",
        "for col in cols_to_check:\n",
        "    if col in df.columns:\n",
        "        print(f\"  {col}: {df[col].dtype}\")\n",
        "\n",
        "# First ensure numeric columns are properly typed\n",
        "print(\"\\nConverting columns to numeric...\")\n",
        "numeric_cols = ['a_total', 'a_deposit', 'payment', 'debt_tot', 'transnum']\n",
        "for col in numeric_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        # Fill missing values with 0 (as per Stata logic)\n",
        "        df[col] = df[col].fillna(0)\n",
        "        print(f\"  Converted {col} to numeric\")\n",
        "\n",
        "# Create customer-level means (equivalent to Stata egen mean, by(id))\n",
        "agg_vars = {\n",
        "    'transactions': 'transnum',\n",
        "    'assets': 'a_total',\n",
        "    'deposits': 'a_deposit', \n",
        "    'paymentmean': 'payment',\n",
        "    'debt': 'debt_tot'\n",
        "}\n",
        "\n",
        "for new_var, source_var in agg_vars.items():\n",
        "    if source_var in df.columns:\n",
        "        customer_means = df.groupby('id')[source_var].mean().reset_index()\n",
        "        customer_means.columns = ['id', new_var]\n",
        "        df = df.merge(customer_means, on='id', how='left')\n",
        "        mean_val = customer_means[new_var].mean()\n",
        "        print(f\"✓ Created {new_var} from {source_var} (mean: {mean_val:.2f})\")\n",
        "\n",
        "# Handle faamount and pastuse\n",
        "# Note: In Stata, egen pastuse = mean(faamount), by(id) creates a variable at observation level\n",
        "# Then collapse (sum) faamount keeps the sum, while pastuse is kept as first value\n",
        "df['faamount'] = df['faamount'].fillna(0)\n",
        "\n",
        "# Create pastuse based on whether customer ever used flex account\n",
        "customer_faamount_sum = df.groupby('id')['faamount'].sum()\n",
        "pastuse_binary = (customer_faamount_sum > 0).astype(int)\n",
        "\n",
        "# Create pastuse at observation level (like Stata egen)\n",
        "df['pastuse'] = df['id'].map(pastuse_binary)\n",
        "\n",
        "print(f\"✓ Created pastuse: {pastuse_binary.sum():,} customers with past usage\")\n",
        "print(f\"  Distribution: {pastuse_binary.value_counts().to_dict()}\")\n",
        "\n",
        "# Handle autobillpay (rename var3)\n",
        "if 'var3' in df.columns:\n",
        "    # First convert var3 to numeric if needed\n",
        "    df['var3'] = pd.to_numeric(df['var3'], errors='coerce').fillna(0)\n",
        "    df['autobillpay_old'] = df['var3'].copy()\n",
        "    \n",
        "    # Create autobillpay mean by customer\n",
        "    autobillpay_mean = df.groupby('id')['autobillpay_old'].mean().reset_index()\n",
        "    autobillpay_mean.columns = ['id', 'autobillpay']\n",
        "    autobillpay_mean['autobillpay'] = (autobillpay_mean['autobillpay'] != 0).astype(int)\n",
        "    df = df.merge(autobillpay_mean, on='id', how='left')\n",
        "    print(f\"✓ Created autobillpay: {autobillpay_mean['autobillpay'].sum():,} customers with autobillpay\")\n",
        "else:\n",
        "    print(\"⚠️  var3 not found - skipping autobillpay creation\")\n",
        "\n",
        "print(\"✓ All aggregated variables created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 4: Selecting variables and collapsing...\n",
            "✓ Selected 19 variables\n",
            "✓ Collapsed to customer level: 108,000 customers\n",
            "  Shape: (108000, 17)\n",
            "✓ Customer count matches expected: 108,000\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Keep only required variables and collapse to customer level\n",
        "print(\"\\nStep 4: Selecting variables and collapsing...\")\n",
        "\n",
        "# Keep only required variables\n",
        "keep_vars = ['id', 'date', 'treatment', 'phase1treat', 'cinsiyet', 'city', 'medenihal', \n",
        "             'falimit', 'autobillpay', 'creditcard', 'credit', 'transactions', 'assets', \n",
        "             'deposits', 'paymentmean', 'debt', 'faamount', 'pastuse', 'acctbalance']\n",
        "\n",
        "available_vars = [v for v in keep_vars if v in df.columns]\n",
        "df = df[available_vars].copy()\n",
        "print(f\"✓ Selected {len(available_vars)} variables\")\n",
        "\n",
        "# Sort before collapse\n",
        "df = df.sort_values(['id', 'date'])\n",
        "\n",
        "# Collapse to customer level\n",
        "# firstnm = first non-missing, lastnm = last non-missing, sum = sum\n",
        "agg_dict = {\n",
        "    'treatment': 'first',\n",
        "    'phase1treat': 'first',\n",
        "    'cinsiyet': 'first',\n",
        "    'city': 'first',\n",
        "    'medenihal': 'first',\n",
        "    'falimit': 'first',\n",
        "    'transactions': 'first',\n",
        "    'assets': 'first',\n",
        "    'deposits': 'first',\n",
        "    'paymentmean': 'first',\n",
        "    'debt': 'first',\n",
        "    'pastuse': 'first',\n",
        "    'autobillpay': 'first',\n",
        "    'creditcard': 'last',\n",
        "    'credit': 'last',\n",
        "    'faamount': 'sum'\n",
        "}\n",
        "\n",
        "# Only aggregate columns that exist\n",
        "available_agg = {k: v for k, v in agg_dict.items() if k in df.columns}\n",
        "df_collapsed = df.groupby('id').agg(available_agg).reset_index()\n",
        "\n",
        "print(f\"✓ Collapsed to customer level: {len(df_collapsed):,} customers\")\n",
        "print(f\"  Shape: {df_collapsed.shape}\")\n",
        "\n",
        "# Check if we have expected 108,000 customers\n",
        "if len(df_collapsed) == 108000:\n",
        "    print(\"✓ Customer count matches expected: 108,000\")\n",
        "else:\n",
        "    print(f\"⚠️  Customer count: {len(df_collapsed):,} (expected: 108,000)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 5: Creating dummy variables...\n",
            "✓ Created city dummies: ['city_0.0', 'city_1.0', 'city_2.0', 'city_3.0', 'city_4.0']\n",
            "✓ Created marital status dummies: ['maritalstatus_1', 'maritalstatus_2', 'maritalstatus_3']\n",
            "✓ Created flex limit dummies: ['flexlim_1.0', 'flexlim_2.0', 'flexlim_3.0']\n",
            "✓ All dummy variables created\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Create dummy variables (Stata ta var, gen(newvar_))\n",
        "print(\"\\nStep 5: Creating dummy variables...\")\n",
        "\n",
        "df = df_collapsed.copy()\n",
        "\n",
        "# City dummies - ta city, gen(city_)\n",
        "city_dummies = pd.get_dummies(df['city'], prefix='city', dtype=int)\n",
        "# Stata labels: city_1 \"Other City\", city_2 \"Istanbul\", etc.\n",
        "# Note: Stata numbering may differ from pandas default\n",
        "df = pd.concat([df, city_dummies], axis=1)\n",
        "print(f\"✓ Created city dummies: {list(city_dummies.columns)}\")\n",
        "\n",
        "# Marital status dummies - ta medenihal, gen(maritalstatus_)\n",
        "marital_dummies = pd.get_dummies(df['medenihal'], prefix='maritalstatus', dtype=int)\n",
        "df = pd.concat([df, marital_dummies], axis=1)\n",
        "print(f\"✓ Created marital status dummies: {list(marital_dummies.columns)}\")\n",
        "\n",
        "# Flex limit dummies - ta falimit, gen(flexlim_)\n",
        "flexlim_dummies = pd.get_dummies(df['falimit'], prefix='flexlim', dtype=int)\n",
        "df = pd.concat([df, flexlim_dummies], axis=1)\n",
        "print(f\"✓ Created flex limit dummies: {list(flexlim_dummies.columns)}\")\n",
        "\n",
        "print(\"✓ All dummy variables created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 6: Creating treatment-related variables...\n",
            "✓ Created all treatment-related variables\n",
            "  Frequency treatments: {1.0: 36052, 0.0: 35985}\n",
            "  Reminder treatments: {1.0: 36064, 2.0: 35973, 0.0: 35963}\n",
            "  Main treatments: {0.0: 18043, 4.0: 18021, 3.0: 17995, 5.0: 17983, 2.0: 17981, 1.0: 17977}\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Create treatment-related variables\n",
        "print(\"\\nStep 6: Creating treatment-related variables...\")\n",
        "\n",
        "# Frequency treatments\n",
        "freq_10_treatments = [2,5,8,11,14,17,20,23,26,29,32,35]\n",
        "freq_20_treatments = [3,6,9,12,15,18,21,24,27,30,33,36]\n",
        "\n",
        "df['freq_10'] = df['treatment'].isin(freq_10_treatments).astype(int)\n",
        "df['freq_20'] = df['treatment'].isin(freq_20_treatments).astype(int)\n",
        "\n",
        "# Create freqtreat variable\n",
        "df['freqtreat'] = np.nan\n",
        "df.loc[df['freq_10'] == 1, 'freqtreat'] = 0\n",
        "df.loc[df['freq_20'] == 1, 'freqtreat'] = 1\n",
        "\n",
        "# Frequency none\n",
        "df['freq_none'] = ((df['freq_10'] == 0) & (df['freq_20'] == 0)).astype(int)\n",
        "\n",
        "# Reminder variables\n",
        "reminder_short_treatments = [2,3,8,9,14,15,20,21,26,27,32,33]\n",
        "df['reminder_short'] = df['treatment'].isin(reminder_short_treatments).astype(int)\n",
        "df['reminder_long'] = ((df['reminder_short'] == 0) & (df['freq_none'] == 0)).astype(int)\n",
        "\n",
        "# Reminder treatment variable\n",
        "df['remindtreat'] = np.nan\n",
        "df.loc[df['freq_none'] == 1, 'remindtreat'] = 0\n",
        "df.loc[df['reminder_short'] == 1, 'remindtreat'] = 1\n",
        "df.loc[df['reminder_long'] == 1, 'remindtreat'] = 2\n",
        "\n",
        "# Main treatment categories\n",
        "df['control'] = ((df['treatment'] >= 1) & (df['treatment'] <= 6)).astype(int)\n",
        "df['faonly'] = ((df['treatment'] >= 31) & (df['treatment'] <= 36)).astype(int)\n",
        "df['fabp'] = ((df['treatment'] >= 25) & (df['treatment'] <= 30)).astype(int)\n",
        "df['fadc'] = ((df['treatment'] >= 13) & (df['treatment'] <= 18)).astype(int)\n",
        "df['bponly'] = ((df['treatment'] >= 7) & (df['treatment'] <= 12)).astype(int)\n",
        "df['dconly'] = ((df['treatment'] >= 19) & (df['treatment'] <= 24)).astype(int)\n",
        "\n",
        "# Combined treatment variable\n",
        "df['treats'] = np.nan\n",
        "df.loc[df['control'] == 1, 'treats'] = 0\n",
        "df.loc[df['faonly'] == 1, 'treats'] = 1\n",
        "df.loc[df['fabp'] == 1, 'treats'] = 2\n",
        "df.loc[df['fadc'] == 1, 'treats'] = 3\n",
        "df.loc[df['bponly'] == 1, 'treats'] = 4\n",
        "df.loc[df['dconly'] == 1, 'treats'] = 5\n",
        "\n",
        "print(\"✓ Created all treatment-related variables\")\n",
        "print(f\"  Frequency treatments: {df['freqtreat'].value_counts().to_dict()}\")\n",
        "print(f\"  Reminder treatments: {df['remindtreat'].value_counts().to_dict()}\")\n",
        "print(f\"  Main treatments: {df['treats'].value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 7: Creating treatment dummy variables...\n",
            "✓ Created 35 treatment dummy variables\n",
            "  Treatment dummies: treat_2 through treat_36\n",
            "✓ Final sort by id\n",
            "\n",
            "Final dataset shape: (108000, 77)\n",
            "Expected shape: (108000, 77)\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Create treatment dummy variables (ta treatment, gen(treat_))\n",
        "print(\"\\nStep 7: Creating treatment dummy variables...\")\n",
        "\n",
        "# Create treatment dummies\n",
        "treatment_dummies = pd.get_dummies(df['treatment'], prefix='treat', dtype=int)\n",
        "\n",
        "# Drop treat_1 (Stata: drop treat_1)\n",
        "if 'treat_1.0' in treatment_dummies.columns:\n",
        "    treatment_dummies = treatment_dummies.drop('treat_1.0', axis=1)\n",
        "elif 'treat_1' in treatment_dummies.columns:\n",
        "    treatment_dummies = treatment_dummies.drop('treat_1', axis=1)\n",
        "\n",
        "# Rename columns to remove .0 suffix if present\n",
        "treatment_dummies.columns = [col.replace('.0', '') for col in treatment_dummies.columns]\n",
        "\n",
        "# Add to main dataframe\n",
        "df = pd.concat([df, treatment_dummies], axis=1)\n",
        "\n",
        "print(f\"✓ Created {len(treatment_dummies.columns)} treatment dummy variables\")\n",
        "print(f\"  Treatment dummies: treat_2 through treat_{36}\")\n",
        "\n",
        "# Final sort by id\n",
        "df = df.sort_values('id')\n",
        "print(\"✓ Final sort by id\")\n",
        "\n",
        "# Update datasets dictionary\n",
        "df_final = df.copy()\n",
        "datasets['orthogonality'] = df_final\n",
        "\n",
        "print(f\"\\nFinal dataset shape: {df_final.shape}\")\n",
        "print(f\"Expected shape: (108000, 77)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "VERIFICATION AGAINST EXISTING ORTHOGONALITY.DTA\n",
            "================================================================================\n",
            "Loading existing orthogonality.dta...\n",
            "\n",
            "Shape comparison:\n",
            "  Generated: (108000, 77)\n",
            "  Existing:  (108000, 77)\n",
            "  Match: ✓\n",
            "\n",
            "Key variable comparison:\n",
            "  ✓ treatment: Perfect match\n",
            "  ✗ pastuse: 8,856 differences\n",
            "  ✗ assets: 107,586 differences\n",
            "  ✗ debt: 37,877 differences\n",
            "  ✓ cinsiyet: Perfect match\n",
            "\n",
            "============================================================\n",
            "⚠️  Dataset created but has some differences from existing\n",
            "   Run additional cells to investigate and fix if needed\n",
            "\n",
            "Final dataset: 108,000 rows × 77 columns\n"
          ]
        }
      ],
      "source": [
        "# Step 8: Verify against existing orthogonality.dta\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"VERIFICATION AGAINST EXISTING ORTHOGONALITY.DTA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load existing dataset for comparison\n",
        "existing_path = config.DATASETS.get('orthogonality')\n",
        "if existing_path and existing_path.exists():\n",
        "    print(f\"Loading existing orthogonality.dta...\")\n",
        "    df_existing, _ = pyreadstat.read_dta(str(existing_path))\n",
        "    \n",
        "    # Basic comparison\n",
        "    print(f\"\\nShape comparison:\")\n",
        "    print(f\"  Generated: {df_final.shape}\")\n",
        "    print(f\"  Existing:  {df_existing.shape}\")\n",
        "    print(f\"  Match: {'✓' if df_final.shape == df_existing.shape else '✗'}\")\n",
        "    \n",
        "    # Column comparison\n",
        "    gen_cols = set(df_final.columns)\n",
        "    exist_cols = set(df_existing.columns)\n",
        "    missing_cols = exist_cols - gen_cols\n",
        "    extra_cols = gen_cols - exist_cols\n",
        "    \n",
        "    if missing_cols or extra_cols:\n",
        "        print(f\"\\nColumn differences:\")\n",
        "        if missing_cols:\n",
        "            print(f\"  Missing: {sorted(missing_cols)}\")\n",
        "        if extra_cols:\n",
        "            print(f\"  Extra: {sorted(extra_cols)}\")\n",
        "    \n",
        "    # Value comparison for key variables\n",
        "    print(f\"\\nKey variable comparison:\")\n",
        "    key_vars = ['treatment', 'pastuse', 'assets', 'debt', 'cinsiyet']\n",
        "    \n",
        "    df_final_sorted = df_final.sort_values('id').reset_index(drop=True)\n",
        "    df_existing_sorted = df_existing.sort_values('id').reset_index(drop=True)\n",
        "    \n",
        "    all_match = True\n",
        "    for var in key_vars:\n",
        "        if var in df_final.columns and var in df_existing.columns:\n",
        "            gen_vals = pd.to_numeric(df_final_sorted[var], errors='coerce')\n",
        "            exist_vals = pd.to_numeric(df_existing_sorted[var], errors='coerce')\n",
        "            \n",
        "            if np.allclose(gen_vals, exist_vals, equal_nan=True, rtol=1e-5):\n",
        "                print(f\"  ✓ {var}: Perfect match\")\n",
        "            else:\n",
        "                diff_count = (~np.isclose(gen_vals, exist_vals, equal_nan=True)).sum()\n",
        "                print(f\"  ✗ {var}: {diff_count:,} differences\")\n",
        "                all_match = False\n",
        "    \n",
        "    # Final assessment\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    if df_final.shape == df_existing.shape and all_match:\n",
        "        print(\"🎉 SUCCESS: Dataset perfectly matches existing orthogonality.dta!\")\n",
        "    else:\n",
        "        print(\"⚠️  Dataset created but has some differences from existing\")\n",
        "        print(\"   Run additional cells to investigate and fix if needed\")\n",
        "    \n",
        "    # Update final dataset\n",
        "    datasets['orthogonality'] = df_final\n",
        "    \n",
        "else:\n",
        "    print(\"No existing orthogonality.dta found for comparison\")\n",
        "    print(\"Generated dataset saved as new reference\")\n",
        "    \n",
        "print(f\"\\nFinal dataset: {df_final.shape[0]:,} rows × {df_final.shape[1]} columns\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
